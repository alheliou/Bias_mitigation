{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alheliou/Bias_mitigation/blob/main/UPP26/TD5_inprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRe6S30b9Ng7"
      },
      "source": [
        "# TD 5: Mitigation des biais avec une méthode de in-processsing Prejudice Remover\n",
        "\n",
        "The aim of this notebook is to use the Prejudice Remover in-processing approach and analyse its impact on the model output.\n",
        "In terms of Machine Learning we will go a bit further in the train/valid/test paradigm.\n",
        "\n",
        "The model has to be learn on the train dataset, then the model parameters has to be optimized on the valid dataset, and finally the model performance is evaluated on the test dataset.\n",
        "No choice/decision etc can be taken depending on the test dataset. This could result on an overfitting on the test dataset.\n",
        "\n",
        "Here you will manipulate:\n",
        "- Prejudice Remover approach as a black box\n",
        "- Training of the prejudice remover using the train/valid paradigm. to choice the 'best' threshold\n",
        "- Combine Prejudice Remover with Reweighing\n",
        "\n",
        "As a reminder of pre-processing approach we encourage you to :\n",
        "- analyse the impact of the Reweighing on different model (Logistic Regression, Decision Tree, Random Forest, etc.)\n",
        "\n",
        "\n",
        "## Installation of the environnement\n",
        "\n",
        "We highly recommend you to follow these steps, it will allow every student to work in an environment as similar as possible to the one used during testing.\n",
        "\n",
        "### Colab Settings\n",
        "  The next cell of code are to execute only once per colab environment\n",
        "\n",
        "\n",
        "#### Python env creation\n",
        "\n",
        "        ```\n",
        "        ! python -m pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
        "        ```\n",
        "### Local Settings\n",
        "\n",
        "#### 1. Uv installation\n",
        "\n",
        "\n",
        "        https://docs.astral.sh/uv/getting-started/installation/\n",
        "\n",
        "\n",
        "        `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
        "\n",
        "        Python version 3.12 installation (highly recommended)\n",
        "        `uv python install 3.12`\n",
        "\n",
        "\n",
        "#### 3. Python env creation\n",
        "\n",
        "        ```\n",
        "        mkdir TD_bias_mitigation\n",
        "        cd TD_bias_mitigation\n",
        "        uv python pin 3.12\n",
        "        uv init\n",
        "        uv pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
        "        ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS-rsfZKD3SC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9ITNS89NhA"
      },
      "source": [
        "## 1. Import and load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88mMZIic9NhB"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "warnings.simplefilter(action=\"ignore\", append=True, category=UserWarning)\n",
        "# Datasets\n",
        "from aif360.datasets import MEPSDataset19\n",
        "\n",
        "# Fairness metrics\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "MEPSDataset19_data = MEPSDataset19()\n",
        "(dataset_orig_panel19_train, dataset_orig_panel19_val, dataset_orig_panel19_test) = (\n",
        "    MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYbdfIPs9NhE"
      },
      "outputs": [],
      "source": [
        "len(dataset_orig_panel19_train.instance_weights), len(\n",
        "    dataset_orig_panel19_val.instance_weights\n",
        "), len(dataset_orig_panel19_test.instance_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5zWlgrRDycU"
      },
      "outputs": [],
      "source": [
        "instance_weights = MEPSDataset19_data.instance_weights\n",
        "instance_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9zwCl6nDycU"
      },
      "outputs": [],
      "source": [
        "f\"Taille du dataset {len(instance_weights)}, poids total du dataset {instance_weights.sum()}.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX6Vk9nIDycU"
      },
      "outputs": [],
      "source": [
        "from aif360.sklearn.metrics import *\n",
        "from sklearn.metrics import  balanced_accuracy_score\n",
        "\n",
        "\n",
        "# This method takes lists\n",
        "def get_metrics(\n",
        "    y_true, # list or np.array of truth values\n",
        "    y_pred=None,  # list or np.array of predictions\n",
        "    prot_attr=None, # list or np.array of protected/sensitive attribute values\n",
        "    priv_group=1, # value taken by the privileged group\n",
        "    pos_label=1, # value taken by the positive truth/prediction\n",
        "    sample_weight=None # list or np.array of weights value,\n",
        "):\n",
        "    group_metrics = {}\n",
        "    group_metrics[\"base_rate_truth\"] = base_rate(\n",
        "        y_true=y_true, pos_label=pos_label, sample_weight=sample_weight\n",
        "    )\n",
        "    group_metrics[\"statistical_parity_difference\"] = statistical_parity_difference(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "    )\n",
        "    group_metrics[\"disparate_impact_ratio\"] = disparate_impact_ratio(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "    )\n",
        "    if not y_pred is None:\n",
        "        group_metrics[\"base_rate_preds\"] = base_rate(\n",
        "        y_true=y_pred, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"equal_opportunity_difference\"] = equal_opportunity_difference(\n",
        "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"average_odds_difference\"] = average_odds_difference(\n",
        "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        if len(set(y_pred))>1:\n",
        "            group_metrics[\"conditional_demographic_disparity\"] = conditional_demographic_disparity(\n",
        "                y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
        "            )\n",
        "        else:\n",
        "            group_metrics[\"conditional_demographic_disparity\"] =None\n",
        "        group_metrics[\"smoothed_edf\"] = smoothed_edf(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"df_bias_amplification\"] = df_bias_amplification(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"balanced_accuracy_score\"] = balanced_accuracy_score(\n",
        "        y_true=y_true, y_pred=y_pred, sample_weight=sample_weight\n",
        "        )\n",
        "    return group_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV0F7tkTDycU"
      },
      "source": [
        "## Learning a Prejudice Remover model on the training dataset, and choose the best parameters with the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dWBZ5RmDycU"
      },
      "outputs": [],
      "source": [
        "# Bias mitigation techniques\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.algorithms.inprocessing import PrejudiceRemover"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlN5mT7VDycU"
      },
      "source": [
        "### Question1 : Learn a Standard Scaler on the training dataset features, its output will be used as input of the model learned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5ZzO-yADycU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0sqqKAcDycU"
      },
      "source": [
        "### Question2: Create a method to learn a Prejudice Remover on the train dataset and retrieve the model learned\n",
        "Execute the method with the parameter eta arbitrarily set at 25.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbVz88N_DycU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2bKnNo_DycU"
      },
      "source": [
        "Le score du Prejudice Remover donne un sortie pour chaque instance une seule valeur, c'est un seuil, arbritrairement fixé à 0.5 par défault, qui permet à partir de ce score de décider la prédiction 1 ou 0.\n",
        "Si le score est supérieur au seuil la prédiction est 1, sinon c'est 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EZ13xMyDycU"
      },
      "source": [
        "### Validating: Choose the best parameters\n",
        "\n",
        "Here there are two parameters :\n",
        "- eta: fairness penalty parameter of the PR model\n",
        "- thershold: the threshold of the binary classification\n",
        "\n",
        "The threshold is used to obtains predictions from the model output.\n",
        "The eta is used during the training\n",
        "\n",
        "Question3: Create a method that will loop over 50 threshold ]0:0.5( and 5 values of ETA [1.0: 100.0], and outputs the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAY1-DfzDycU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3XbHo41DycU"
      },
      "source": [
        "### Question4 : Make plot to choose the best set of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqwWdgVVDycU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0K7A5u_DycU"
      },
      "source": [
        "### Question 5: Evaluate : compute the metrics on the test dataset using the model learnt with the selected parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVii7zd7DycU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9vzL4RaDycU"
      },
      "source": [
        "## Combine pre-processing and in-processing\n",
        "### Question6: Redo the Prejudice Remover approach using first the Reweighing pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyCLr_UqDycV"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILgpsj0aDycV"
      },
      "source": [
        "## Adversarial Debiasing\n",
        "\n",
        "Adversarial debiasing [1] is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions.\n",
        "\n",
        "See [AIF360 tuto](https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_adversarial_debiasing.ipynb)\n",
        "\n",
        "Here we show how to learn and Adversarial Debiasing with the argumetn debias set to False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgA5dwrdDycV"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "plain_model = AdversarialDebiasing(\n",
        "    unprivileged_groups=[{'RACE': 0.0}],\n",
        "    privileged_groups=[{'RACE': 1.0}],\n",
        "    scope_name='plain_classifier',\n",
        "    debias=False,\n",
        "    sess=sess)\n",
        "\n",
        "plain_model.fit(dataset_orig_panel19_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pgJF7gwDycV"
      },
      "outputs": [],
      "source": [
        "# Apply the plain model to train and val data\n",
        "dataset_nodebiasing_train = plain_model.predict(dataset_orig_panel19_train)\n",
        "dataset_nodebiasing_val = plain_model.predict(dataset_orig_panel19_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPXEKWEyDycV"
      },
      "outputs": [],
      "source": [
        "get_metrics(\n",
        "    y_true = dataset_orig_panel19_train.labels[:,0],\n",
        "    y_pred= dataset_nodebiasing_train.labels[:,0],\n",
        "    prot_attr= dataset_orig_panel19_train.protected_attributes[:,0],\n",
        "    sample_weight= dataset_orig_panel19_train.instance_weights\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1sU9551DycV"
      },
      "outputs": [],
      "source": [
        "get_metrics(\n",
        "    y_true = dataset_orig_panel19_val.labels[:,0],\n",
        "    y_pred= dataset_nodebiasing_val.labels[:,0],\n",
        "    prot_attr= dataset_orig_panel19_val.protected_attributes[:,0],\n",
        "    sample_weight= dataset_orig_panel19_val.instance_weights\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOxGv1r0DycV"
      },
      "outputs": [],
      "source": [
        "sess.close()\n",
        "tf.reset_default_graph()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6nSHAJBDycV"
      },
      "source": [
        "### Question 7: Redo the same (learn and Adversarial Debiasing) with the argument debias set to True\n",
        "\n",
        "Compare the metrics outputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbcOJi97DycV"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ7CV8NgDycV"
      },
      "source": [
        "### Question 8: Combine the Reweighing with the Adversarial Debiasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koiYRffwDycV"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE0pd5MkDycV"
      },
      "source": [
        "This in-processing approach does not seem compatible withe the Reweighing, has the df_bias_amplification is high and the disparate impact ratio is not improved by the use of the reweighing has pre-processing.\n",
        "Although very efficient on the fairness metrics of the dataset, the Reweighing is not convenient for every kind of machine learning algo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nashEEuDDycV"
      },
      "source": [
        "## Analysis of the influence of Reweighing\n",
        "\n",
        "### QUESTION 9 : Pour aller plus loin, étudier l'impact du Reweighing sur différents modèles notamment les arbres de décision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GKirdT6DycV"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTpxnNpaDycV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "TD_corr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}